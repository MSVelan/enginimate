{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e38013a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import voyageai\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Set your API key (store it in an env var for safety)\n",
    "os.environ[\"VOYAGE_API_KEY\"] = os.getenv(\"VOYAGE_KEY\")\n",
    "client = voyageai.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "714b9e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_voyage(texts: list[str]) -> list[list[float]]:\n",
    "    \"\"\"\n",
    "    Calls Voyage's /embeddings endpoint with model `voyage-code-3`.\n",
    "    Returns a list of 1024‑dim float vectors.\n",
    "    \"\"\"\n",
    "    # Voyage automatically truncates to 2048 tokens; we pre‑split to be safe.\n",
    "    resp = client.embed(\n",
    "        model=\"voyage-code-3\",\n",
    "        input=texts,          # can be a list of strings (batch up to 64)\n",
    "    )\n",
    "    return resp.embeddings   # already a List[List[float]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "375a75be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "ENC = tiktoken.get_encoding(\"cl100k_base\")\n",
    "MAX_TOKENS = 2048\n",
    "\n",
    "def split_to_chunks(text: str, max_tokens: int = MAX_TOKENS) -> list[str]:\n",
    "    token_ids = ENC.encode(text)\n",
    "    chunks = []\n",
    "    for i in range(0, len(token_ids), max_tokens):\n",
    "        chunk_ids = token_ids[i:i+max_tokens]\n",
    "        chunks.append(ENC.decode(chunk_ids))\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eabf839",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib, ast\n",
    "from docutils.core import publish_doctree\n",
    "\n",
    "def rst_chunks(file_path: pathlib.Path) -> list[dict]:\n",
    "    text = file_path.read_text(encoding=\"utf-8\")\n",
    "    doctree = publish_doctree(text)\n",
    "\n",
    "    chunks = []\n",
    "    for node in doctree.traverse():\n",
    "        if node.tagname == \"literal_block\":                     # code block\n",
    "            chunks.append({\n",
    "                \"text\": node.astext(),\n",
    "                \"type\": \"code\",\n",
    "                \"file\": str(file_path),\n",
    "                \"line_start\": node.line,\n",
    "                \"line_end\": node.line + node.astext().count(\"\\n\"),\n",
    "            })\n",
    "        elif node.tagname in (\"paragraph\", \"title\", \"section\"):\n",
    "            # Split large paragraphs into token‑bounded windows\n",
    "            for chunk_text in split_to_chunks(node.astext()):\n",
    "                chunks.append({\n",
    "                    \"text\": chunk_text,\n",
    "                    \"type\": \"doc\",\n",
    "                    \"file\": str(file_path),\n",
    "                    \"line_start\": node.line,\n",
    "                    \"line_end\": node.line + chunk_text.count(\"\\n\"),\n",
    "                })\n",
    "    return chunks\n",
    "\n",
    "def py_chunks(file_path: pathlib.Path) -> list[dict]:\n",
    "    source = file_path.read_text(encoding=\"utf-8\")\n",
    "    tree = ast.parse(source, filename=str(file_path))\n",
    "\n",
    "    chunks = []\n",
    "    for node in ast.iter_child_nodes(tree):\n",
    "        if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):\n",
    "            start = node.lineno - 1\n",
    "            end   = node.end_lineno\n",
    "            code  = \"\\n\".join(source.splitlines()[start:end])\n",
    "            chunks.append({\n",
    "                \"text\": code,\n",
    "                \"type\": \"code\",\n",
    "                \"file\": str(file_path),\n",
    "                \"line_start\": start + 1,\n",
    "                \"line_end\": end,\n",
    "            })\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e044e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_all_chunks(root_dir: pathlib.Path) -> list[dict]:\n",
    "    chunks = []\n",
    "    for path in root_dir.rglob(\"*\"):\n",
    "        if path.suffix == \".rst\":\n",
    "            chunks.extend(rst_chunks(path))\n",
    "        elif path.suffix == \".py\":\n",
    "            chunks.extend(py_chunks(path))\n",
    "    return chunks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enginimate",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
